[
  {
    "path": "tidy/2022-09-11-tidy-tuesday-4/",
    "title": "Tidy Tuesday 10-26-2021",
    "description": "Marathon Running for each age group.",
    "author": [],
    "date": "2021-10-26",
    "categories": [],
    "contents": "\n\n\n# Or read in the data manually\n\nultra_rankings <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-26/ultra_rankings.csv')\nrace <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-26/race.csv')\n\n\n\nHere are the datasets if you are interested:\n\n\nultra_rankings\n\n\n# A tibble: 137,803 × 8\n   race_year_id  rank runner             time    age gender nationality\n          <dbl> <dbl> <chr>              <chr> <dbl> <chr>  <chr>      \n 1        68140     1 VERHEUL Jasper     26H …    30 M      GBR        \n 2        68140     2 MOULDING JON       27H …    43 M      GBR        \n 3        68140     3 RICHARDSON Phill   28H …    38 M      GBR        \n 4        68140     4 DYSON Fiona        30H …    55 W      GBR        \n 5        68140     5 FRONTERAS Karen    32H …    48 W      GBR        \n 6        68140     6 THOMAS Leigh       32H …    31 M      GBR        \n 7        68140     7 SHORT Deborah      33H …    55 W      GBR        \n 8        68140     8 CROSSLEY Catharine 33H …    40 W      GBR        \n 9        68140     9 BUTCHER Kent       34H …    47 M      GBR        \n10        68140    10 Hendry Bill        34H …    29 M      GBR        \n# … with 137,793 more rows, and 1 more variable:\n#   time_in_seconds <dbl>\n\n\n\nrace\n\n\n# A tibble: 1,207 × 13\n   race_year_id event     race    city   country date       start_time\n          <dbl> <chr>     <chr>   <chr>  <chr>   <date>     <time>    \n 1        68140 Peak Dis… Millst… Castl… United… 2021-09-03 19:00     \n 2        72496 UTMB®     UTMB®   Chamo… France  2021-08-27 17:00     \n 3        69855 Grand Ra… Ultra … viell… France  2021-08-20 05:00     \n 4        67856 Persenk … PERSEN… Aseno… Bulgar… 2021-08-20 18:00     \n 5        70469 Runfire … 100 Mi… uluki… Turkey  2021-08-20 18:00     \n 6        66887 Swiss Al… 160KM   Münst… Switze… 2021-08-15 17:00     \n 7        67851 Salomon … Salomo… Folld… Norway  2021-08-14 07:00     \n 8        68241 Ultra Tr… 160KM   Spa    Belgium 2021-08-14 07:00     \n 9        70241 Québec M… QMT-10… Beaup… Canada  2021-08-13 22:00     \n10        69945 Bunketor… BBUT -… LINDO… Sweden  2021-08-07 10:00     \n# … with 1,197 more rows, and 6 more variables: participation <chr>,\n#   distance <dbl>, elevation_gain <dbl>, elevation_loss <dbl>,\n#   aid_stations <dbl>, participants <dbl>\n\n\n\nlibrary(tidyverse)         # for reading in data, graphing, and cleaning\nlibrary(tidymodels)        # for modeling... tidily\nlibrary(usemodels)         # for suggesting step_XXX() functions\nlibrary(glmnet)            # for regularized regression, including LASSO\nlibrary(naniar)            # for examining missing values (NAs)\nlibrary(lubridate)         # for date manipulation\nlibrary(moderndive)        # for King County housing data\nlibrary(vip)               # for variable importance plots\n#library(rmarkdown)         # for paged tables\nlibrary(stacks)\nlibrary(doParallel)        # for parallel processing\ntheme_set(theme_minimal()) # my favorite ggplot2 theme :)\nlibrary(ggthemes)\nlibrary(hms)\n\n\n\nFor my interest, this week, I will try to find the average running\ntime for each age group across all countries. The way I am doing that is\nfirst by filtering the age group: only from 10 and above. After that, I\nwill try to group by each age group and gender, followed by finding the\nmean running time for each of the group.\nFollowing with the data wrangling, I graph the information using the\nline graph to fully see the differences.\n\n\nultra_rankings %>% \n  filter(age > 10, age < 100) %>%\n  select(rank, age, time_in_seconds, gender) %>% \n  drop_na() %>% \n  mutate(time_Hour = time_in_seconds / 3600) %>% \n  mutate(age_groups = cut(age, seq(10, 140, 10))) %>% \n  group_by(age_groups, gender) %>% \n  summarize(mean_time = mean(time_Hour)) %>% \n  ggplot(aes(x= age_groups, y = mean_time, fill = gender)) + \n  geom_col(position = \"dodge\") +\n  geom_text(aes(x = age_groups, y =mean_time, label = sprintf(\"%0.2f\", round(mean_time, digits = 2))),\n            position = position_dodge(width = 0.8), \n            vjust = -1, size = 2,\n            fontface=\"bold\", \n            family=\"Times\") +\n  scale_fill_brewer(palette=\"Dark2\") + \n  labs(title = \"Average running time in Hour for each age group accross all countries\", \n       y= NULL, \n       x = NULL) + \n  theme(axis.title.x = element_blank(), \n        axis.title.y = element_blank(),\n        plot.title = element_text(family = \"Times\"), \n        axis.text = element_blank()) + \n  theme_minimal()\n\n\n\n\n\n\n\n",
    "preview": "tidy/2022-09-11-tidy-tuesday-4/tidy-tuesday-4_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-09-11T18:26:32-05:00",
    "input_file": {}
  },
  {
    "path": "tidy/2022-09-11-tidy-tuesday-3/",
    "title": "Recreation Graph Challenge Tidy Tuesday",
    "description": "Recreation Dubois Challege",
    "author": [],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\nFor this week, the data I faced is completely different. It is part\nof the #DuBoisChallenge where you will attempt to recreate\nvisualizations made by W.E.B. DuBois for the 1900 Paris Exposition. This\nweek, the goal for is to pay close attention, do background reading and\ntry to recreate it as close as possible.\nIf you want to read about the summary for the TidyTuesday for this\nweek, check out this\npage.\nFor the interested graphs to recreate, you can find in this resource,\nwhich displays many of the graphs.\nFor this challenge, I am going to recreate the Georgia graph:\n\n\n\n\n\n\n\n\n\n",
    "preview": "tidy/2022-09-11-tidy-tuesday-3/tidy-tuesday-3_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-09-11T17:45:10-05:00",
    "input_file": {}
  },
  {
    "path": "tidy/2022-09-11-tidy-tuesday-5/",
    "title": "Tidy Tuesday 10-19-2021",
    "description": "Which vegetables got cumulated the most?",
    "author": [],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\nLoad in the library:\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gghighlight)\nlibrary(googlesheets4)\nlibrary(lubridate)\n\n\n\nHere is the dataset. The dataset is actually a garden from my\nprofessor: Lisa Lendway. Here, I am trying to find her favorite\nvegetables!\n\n\ngs4_deauth() #To not have to authorize each time you knit.\n\nharvest_2021 <- read_sheet(\"https://docs.google.com/spreadsheets/d/1DU6dpxrbPGW2oJHQ6TG_xibazoGhvmRcqk2iPGOaRUI/edit?usp=sharing\") %>%\nmutate(date = ymd(date))\n\n\n\n\n\nharvest_2021\n\n\n# A tibble: 726 × 5\n   vegetable variety   date       weight units\n   <chr>     <chr>     <date>      <dbl> <chr>\n 1 asparagus perennial 2021-05-10     32 grams\n 2 lettuce   volunteer 2021-05-10     19 grams\n 3 lettuce   volunteer 2021-05-19     91 grams\n 4 lettuce   volunteer 2021-05-19    125 grams\n 5 lettuce   volunteer 2021-05-20     79 grams\n 6 lettuce   volunteer 2021-05-20     74 grams\n 7 lettuce   volunteer 2021-05-21     18 grams\n 8 lettuce   volunteer 2021-05-21     65 grams\n 9 lettuce   volunteer 2021-05-22     61 grams\n10 lettuce   volunteer 2021-05-23     82 grams\n# … with 716 more rows\n\nIn here, I am trying to explore the min and the max date for the\nplot:\n\n\nharvest_2021 %>% \n  summarise(min(date), max(date))\n\n\n# A tibble: 1 × 2\n  `min(date)` `max(date)`\n  <date>      <date>     \n1 2021-05-10  2021-11-10 \n\nFor this week, my tidy tuesday will be trying to find which\nvegetables have the highest cumulative weight from May 2021 to October\n2021.\n\n\nharvest_2021 %>% \n  mutate(wt_lbs = weight*0.00220462) %>% \n  group_by(vegetable, date) %>%\n  summarize(daily_wt_lbs = sum(wt_lbs)) %>% \n  mutate(cum_wt_lbs = cumsum(daily_wt_lbs)) %>% \n  arrange(cum_wt_lbs) %>% \n  ggplot(aes(x = date, y = cum_wt_lbs, color = vegetable)) +\n  geom_line() + \n  gghighlight(max(cum_wt_lbs) > 90) + \n  labs(title = \"The vegatable's cumulative weight in pounds from May 2021 to October 2021\",\n       y = \"\",\n       x = \"\") + \n  theme_minimal() + \n  theme(# remove the vertical grid lines\n        panel.grid.major.x = element_blank() ,\n        panel.grid.major.y = element_line( size=.05, color=\"black\")) \n\n\n\n\n\n\n\n",
    "preview": "tidy/2022-09-11-tidy-tuesday-5/tidy-tuesday-5_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-09-11T18:31:55-05:00",
    "input_file": {}
  },
  {
    "path": "tidy/2022-09-07-tidy-tuesday-1/",
    "title": "Tidy Tuesday 09-21-2021",
    "description": "My attempt for tidy tuesday 09-21-21. The dataset is about the Emmy Awards.",
    "author": [],
    "date": "2021-09-21",
    "categories": [],
    "contents": "\n\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)         # for graphing and data cleaning\n\n\n\n\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata <- tidytuesdayR::tt_load('2021-09-21')\n\n\n\n    Downloading file 1 of 1: `nominees.csv`\n\ntuesdata <- tidytuesdayR::tt_load(2021, week = 39)\n\n\n\n    Downloading file 1 of 1: `nominees.csv`\n\nnominees <- tuesdata$nominees\n\n# Or read in the data manually\n\nnominees <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-21/nominees.csv')\n\n\n\n\n\n### Create a graph based on every category\nnominees_types <- nominees %>%\n  filter(year > 2010) %>%\n  group_by(year, type) %>%\n  summarize(count=n()) \n\n\nnominees_types %>%\n  ggplot(aes(x = as.factor(year), y = count, fill = type)) + geom_col(position = \"dodge\") + \n  geom_text(aes(x = as.factor(year), y = count, label = count),\n            position = position_dodge(width = 0.8), \n            vjust = -1, size = 2,\n            fontface=\"bold\", \n            family=\"Times\") + \n  labs(title = \"Emmy Awards Nominee and Winner from 2011 to 2021\") + \n  theme_minimal() + \n  theme(axis.title.x = element_blank(), \n        axis.title.y = element_blank(),\n        plot.title = element_text(family = \"Times\"))\n\n\n\n\nIn here, we can clearly see that from 2011 to 2013, the number of\nnominee increased rapidly. However, the number of nominee in 2014\nsignificantly dropped.\nAfter a strange 2014, the number of nominee increased significantly\nevery year from 1630 nominees in 2015 to 2152 nominees in 2019.\nMeanwhile, the number of winner remains relatively stable.\n\n\n\n",
    "preview": "tidy/2022-09-07-tidy-tuesday-1/tidy-tuesday-1_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-09-07T18:13:24-05:00",
    "input_file": {}
  },
  {
    "path": "tidy/2022-09-11-tidy-tuesday-2/",
    "title": "Tidy Tuesday 05-10-2021",
    "description": "For this week, I am trying to find information about nurses in the US: their salary, how many nurses are there for each state in 2020.",
    "author": [],
    "date": "2021-05-10",
    "categories": [],
    "contents": "\n\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)         # for graphing and data cleaning\nlibrary(ggthemes)\nlibrary(ggplot2) # tidyverse data visualization package\n\n\n\n\n\nlibrary(ggmap)\nlibrary(tmaptools)\nlibrary(maps)\nlibrary(plotly)\nlibrary(scales)\n\n\n\n\n\nlibrary(readr)\nstatePopulation <- read_delim(\"statePopulation.csv\", \n    delim = \";\", escape_double = FALSE, trim_ws = TRUE)\n\n\n\n\n\nnurses <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-05/nurses.csv')\n\n\n\n\n\nnurses_join <- nurses %>% \n  left_join(statePopulation, by = \"State\" )\n\nnurses_join\n\n\n# A tibble: 1,242 × 24\n   State                 Year `Total Employed… `Employed Stand… `Hourly Wage Av…\n   <chr>                <dbl>            <dbl>            <dbl>            <dbl>\n 1 Alabama               2020            48850              2.9             29.0\n 2 Alaska                2020             6240             13               45.8\n 3 Arizona               2020            55520              3.7             38.6\n 4 Arkansas              2020            25300              4.2             30.6\n 5 California            2020           307060              2               58.0\n 6 Colorado              2020            52330              2.8             37.4\n 7 Connecticut           2020            33400              6.5             40.8\n 8 Delaware              2020            11410             11.4             35.7\n 9 District of Columbia  2020            10320              1.2             43.3\n10 Florida               2020           183130              2.2             33.4\n# … with 1,232 more rows, and 19 more variables:\n#   Hourly Wage Median <dbl>, Annual Salary Avg <dbl>,\n#   Annual Salary Median <dbl>, Wage/Salary standard error (%) <dbl>,\n#   Hourly 10th Percentile <dbl>, Hourly 25th Percentile <dbl>,\n#   Hourly 75th Percentile <dbl>, Hourly 90th Percentile <dbl>,\n#   Annual 10th Percentile <dbl>, Annual 25th Percentile <dbl>,\n#   Annual 75th Percentile <dbl>, Annual 90th Percentile <dbl>, …\n\nFor this week’s data, I can see that we are having nurse’s data in 50\nstates in the US in 2020. I can see their wages by weekly, monthly and\nyearly. Additionally, I can see the states’ ranking for the nurse\nprofession as well!\nSo, my tidy tuesday will be: Try to focus on 2020 only, try to find\nthe salary as well as the number of registered nurse for each state.\nThe first thing I did is to filter by the year of 2020. After that, I\ntried to created a map featuring the number of nurses as well as the\nmedian salary for the US.\n\n\nnurses_2020 <- nurses_join %>% \n  filter(Year == 2020) %>% \n  mutate(std_registered_per_1000 = `Total Employed RN` / Pop * 1000) %>% \n  mutate(`Registered Nurses per 1000 people` = std_registered_per_1000) %>% \n  mutate(state_lower = tolower(State)) \n  \nnurses_2020\n\n\n# A tibble: 54 × 27\n   State                 Year `Total Employed… `Employed Stand… `Hourly Wage Av…\n   <chr>                <dbl>            <dbl>            <dbl>            <dbl>\n 1 Alabama               2020            48850              2.9             29.0\n 2 Alaska                2020             6240             13               45.8\n 3 Arizona               2020            55520              3.7             38.6\n 4 Arkansas              2020            25300              4.2             30.6\n 5 California            2020           307060              2               58.0\n 6 Colorado              2020            52330              2.8             37.4\n 7 Connecticut           2020            33400              6.5             40.8\n 8 Delaware              2020            11410             11.4             35.7\n 9 District of Columbia  2020            10320              1.2             43.3\n10 Florida               2020           183130              2.2             33.4\n# … with 44 more rows, and 22 more variables:\n#   Hourly Wage Median <dbl>, Annual Salary Avg <dbl>,\n#   Annual Salary Median <dbl>, Wage/Salary standard error (%) <dbl>,\n#   Hourly 10th Percentile <dbl>, Hourly 25th Percentile <dbl>,\n#   Hourly 75th Percentile <dbl>, Hourly 90th Percentile <dbl>,\n#   Annual 10th Percentile <dbl>, Annual 25th Percentile <dbl>,\n#   Annual 75th Percentile <dbl>, Annual 90th Percentile <dbl>, …\n\n\n\nstates_map <- map_data(\"state\")\n\n\n\n\n\nnurses_median_salary <- nurses %>% \n  filter(Year == 2020) %>% \n  mutate(state_lower = tolower(State)) %>% \n  ggplot(aes(fill = `Annual Salary Median`)) + \n  geom_map(aes(map_id = state_lower), color = \"gray\", size = 0.3, map = states_map) + \n  expand_limits (x = states_map$long, y = states_map$lat) + \n  theme_map() + \n  scale_fill_gradient(low = \"floralwhite\", high = \"firebrick4\")\n\nggplotly(nurses_median_salary)\n\n\n\n\nThe second map I am going to create will be how many registered\nnurses per 1000 people:\n\n\nnurses_map <- nurses_2020 %>% \n  ggplot(aes(fill = `Registered Nurses per 1000 people`, label = State)) + \n  geom_map(aes(map_id = state_lower), color = \"gray\", size = 0.3, map = states_map) + \n  expand_limits (x = states_map$long, y = states_map$lat) +\n  theme_map() + \n  scale_fill_gradient(low = \"white\", high = \"navy\") + labs (fill = \"Registered Nurses per 1000 people\")\n\nggplotly(nurses_map)\n\n\n\n\n\n\nlibrary(ggpubr)\n\nggarrange(nurses_median_salary, nurses_map,\n          ncol = 2, nrow = 1)\n\n\n\n\nLooking at the graph, you can clearly see the differences between the\nsalary for each region. Moreover, we can see that California, Oregon, or\nWashington have the highest median annual salary. On the other hand,\nMinnesota, North or South Dakota have the highest number of registered\nnurses per 1000 people, probably because of low population compared to\nother regions in the US.\n\n\n\n",
    "preview": "tidy/2022-09-11-tidy-tuesday-2/map.png",
    "last_modified": "2022-09-11T18:20:08-05:00",
    "input_file": {}
  }
]
